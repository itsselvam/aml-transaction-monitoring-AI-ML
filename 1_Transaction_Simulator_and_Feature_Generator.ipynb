{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation for Transactions, Anomalies and generated Features\n",
    "\n",
    "Paper: A. Elliott, M. Cucuringu, M. M. Luaces, P. Reidy, and G. Reinert, Anomaly detection in networks with application to financial transaction networks, 2019\n",
    "\n",
    "Based on the open source: https://github.com/zhangcheng1006/Anomaly-Detection-in-Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(message)s', level=logging.INFO)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from numpyencoder import NumpyEncoder\n",
    "from networkx.readwrite import json_graph\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating artificial networks and node features\n",
    "This section is a demonstration of how to generate artificial networks and node features. Please to put all python scripts implementing the tool functions under the same directory of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rpy2.robjects.packages import importr\n",
    "devtools = importr('devtools')\n",
    "# devtools.install_github(\"dynverse/netdist\", dependencies = True)\n",
    "# devtools.install_github(\"alan-turing-institute/network-comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ssss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import generate_null_models, get_parameters\n",
    "from generator import ER_generator, draw_anomalies\n",
    "from basic_test import basic_features\n",
    "from com_detection import community_detection\n",
    "from spectral_localisation import spectral_features\n",
    "from NetEMD import NetEMD_features\n",
    "from path_finder import path_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set meta-parameters for network generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_models = 20     # original = 20\n",
    "num_nodes = 1000    # original = 1000\n",
    "num_basic_mc_samples = 500  # original = 500\n",
    "num_references = 10     # original = 10\n",
    "num_null_models = 60    # original = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set 2 important parameters ```(p, w)```. ```p``` determines the edge density of the network, ```1-w``` is the under boundary of weight of the added anomaly edges.\n",
    "Here all chosen ```(p, w)``` must satisfy the detectability constraints (see equations (11)-(15) in the paper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = np.linspace(0.001, 0.05, 50)\n",
    "ws = np.linspace(0.0, 0.01, 11)\n",
    "candidate_parameters = get_parameters(num_nodes, ps, ws)\n",
    "num_cand_param = len(candidate_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AML_TYPE_DICT = {None: 0, 'path': 1, 'star': 2, 'ring': 3, 'clique': 4, 'tree': 5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate ```num_models``` models with randomly chosen parameters ```(p, w)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feature_graph(model_id, p, w):\n",
    "    # p, w = candidate_parameters[np.random.choice(range(num_cand_param))]\n",
    "    logging.info(\"Computing {}-th/{} model (p={:.3f}, w={:.3f})\".format(model_id, num_models, p, w))\n",
    "    graph = ER_generator(n=num_nodes, p=p, seed=None)\n",
    "    graph = draw_anomalies(graph, w=1 - w)\n",
    "    graph = calculate_graph_features(graph)\n",
    "    return graph\n",
    "\n",
    "\n",
    "def calculate_graph_features(graph):\n",
    "    logging.info(\"\\n\\nGenerating null models 1\\n\\n\")\n",
    "    _, references = generate_null_models(graph, num_models=num_references, min_size=10)     # min_size=20 original\n",
    "    logging.info(\"\\n\\nGenerating null models 2\\n\\n\")\n",
    "    null_samples_whole, null_samples = generate_null_models(graph, num_models=num_null_models, min_size=20)\n",
    "    logging.info(\"\\n\\nGenerating NetEMD features\\n\\n\")\n",
    "    graph = NetEMD_features(graph, references, null_samples, num_references=num_references, num_samples=num_null_models)\n",
    "    logging.info(\"\\n\\nGenerating basic features\\n\\n\")\n",
    "    graph = basic_features(graph, num_samples=num_basic_mc_samples)\n",
    "    logging.info(\"\\n\\nGenerating community features\\n\\n\")\n",
    "    graph = community_detection(graph, null_samples, num_samples=20)\n",
    "    logging.info(\"\\n\\nGenerating spectral features\\n\\n\")\n",
    "    graph = spectral_features(graph, null_samples, num_samples=num_null_models)\n",
    "    logging.info(\"\\n\\nGenerating path features\\n\\n\")\n",
    "    graph = path_features(graph, null_samples_whole, num_samples=num_null_models)\n",
    "    return graph\n",
    "\n",
    "\n",
    "def write_json_graph(graph, model_id, p, w):\n",
    "    data = json_graph.node_link_data(graph)\n",
    "    with open('./data_small_feature_graph/Network_p_{:.3f}_w_{:.3f}_{}.json'.format(p, w, model_id), 'w') as outfile:\n",
    "        json.dump(data, outfile, cls=NumpyEncoder)\n",
    "\n",
    "\n",
    "def write_csv_df(graph, model_id, p, w):\n",
    "    features = set()\n",
    "    for node in graph.nodes():\n",
    "        features |= set(graph.nodes[node].keys())\n",
    "    # features.remove('type')\n",
    "    logging.info(\"\\n\\nComposing DataFrame\\n\\n\")\n",
    "    X = pd.DataFrame.from_dict(dict(graph.nodes(data=True, default=0)), orient='index')\n",
    "    X.fillna(0, inplace=True)\n",
    "    X.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    logging.info(\"\\n\\nWriting to local file\\n\\n\")\n",
    "    X.to_csv('./data_small_feature_graph/Network_p_{:.3f}_w_{:.3f}_{}.csv'.format(p, w, model_id))\n",
    "    \n",
    "\n",
    "def generate_multiple_graph_to_json_and_csv():\n",
    "    for model_id in range(num_models):\n",
    "        p, w = candidate_parameters[np.random.choice(range(num_cand_param))]\n",
    "        graph = generate_feature_graph(model_id, p, w)\n",
    "        write_json_graph(graph, model_id, p, w)\n",
    "        write_csv_df(graph, model_id, p, w)\n",
    "\n",
    "\n",
    "def generate_graph_dataset_json_for_fastgcn(model_id):\n",
    "    p, w = candidate_parameters[np.random.choice(range(num_cand_param))]\n",
    "    graph = generate_feature_graph(model_id, p, w)\n",
    "    data = json_graph.node_link_data(graph)\n",
    "    with open('data_run_test/big_graph_50k.json', 'w') as outfile:\n",
    "        json.dump(data, outfile, cls=NumpyEncoder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Generating Feature Graph__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "print('starting...................................: ', start)\n",
    "\n",
    "generate_multiple_graph_to_json_and_csv()\n",
    "# generate_graph_dataset_json_for_fastgcn(4)\n",
    "\n",
    "end = datetime.now()\n",
    "print('starting...................................: ', start)\n",
    "print('finish.....................................: ', end)\n",
    "print('duration...................................: ', (end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Discussion__:\n",
    "\n",
    "In the paper, the authors test the model trained on the training set generated a specific parameter on the testing set generated by the same parameter. However, in pratice, we can never know how the testing network is generated. To overcome this problem, we generate networks with random parameters and hide this information during the whole procedure, which means we know nothing about the parameter neither on the training set nor the testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform graph to other formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_class_map_json(G, path, file_name):\n",
    "    class_map_json = {n: AML_TYPE_DICT[(G.nodes[n]).get(\"type\", None)] for n in G.nodes()}\n",
    "    with open(path + file_name + '-class_map.json', 'w') as outfile:\n",
    "        json.dump(class_map_json, outfile, cls=NumpyEncoder)\n",
    "\n",
    "\n",
    "def create_id_map_json(G, path, file_name):\n",
    "    id_map_json = {n: ind for ind, n in enumerate(G.nodes())}\n",
    "    with open(path + file_name + '-id_map.json', 'w') as outfile:\n",
    "        json.dump(id_map_json, outfile, cls=NumpyEncoder)\n",
    "\n",
    "\n",
    "def create_feats_npy(G, path, file_name):\n",
    "    g_df = pd.DataFrame.from_dict(G.nodes, orient='index')\n",
    "    g_df = g_df.fillna(0)\n",
    "    g_df = g_df.drop('type', axis=1)\n",
    "    feats = g_df.to_numpy()  # get df after removing type and index columns\n",
    "    np.save(path + file_name + '-feats.npy', feats)\n",
    "\n",
    "\n",
    "def create_train_val_test_graph(G, path, file_name):\n",
    "    mapping = dict(zip(G.nodes(), map(str, G.nodes())))\n",
    "    G = nx.relabel_nodes(G, mapping)\n",
    "\n",
    "    class_map_json = {n: AML_TYPE_DICT[G[n].get(\"type\", None)] for n in G.nodes()}\n",
    "    x = list(class_map_json.keys())\n",
    "    y = list(class_map_json.values())\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=.2, random_state=0, stratify=y)\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, train_size=.50, random_state=0, stratify=y_train)\n",
    "\n",
    "    for n in G.nodes():\n",
    "        G.nodes[n]['test'] = False\n",
    "        G.nodes[n]['val'] = False\n",
    "    for n in x_train:\n",
    "        G.nodes[n]['test'] = True\n",
    "    for n in x_val:\n",
    "        G.nodes[n]['val'] = True\n",
    "\n",
    "    data = json_graph.node_link_data(G)\n",
    "    with open(path + file_name + '-updated.json', 'w') as outfile:\n",
    "        json.dump(data, outfile, cls=NumpyEncoder)\n",
    "\n",
    "\n",
    "def standard_graph_to_multiple_datasource(path, file_name):\n",
    "    G = json_graph.node_link_graph(json.load(open(path + file_name + '.json')))\n",
    "    create_class_map_json(G, path, file_name)\n",
    "    create_id_map_json(G, path, file_name)\n",
    "    create_feats_npy(G, path, file_name)\n",
    "    create_train_val_test_graph(G, path, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Transforming Feature Graph__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "print('starting...................................: ', start)\n",
    "\n",
    "standard_graph_to_multiple_datasource('data_fastgcn/input/', 'Network_p_0.016_w_0.003_1')\n",
    "\n",
    "end = datetime.now()\n",
    "print('starting...................................: ', start)\n",
    "print('finish.....................................: ', end)\n",
    "print('duration...................................: ', (end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform dataframe to graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_data_format(dataset_dir, output_file):\n",
    "    G = json_graph.node_link_graph(json.load(open(dataset_dir + \"/Network_p_0.016_w_0.003_1-updated.json\")))\n",
    "    labels = json.load(open(dataset_dir + \"/Network_p_0.016_w_0.003_1-class_map.json\"))\n",
    "\n",
    "    train_ids = [n for n in G.nodes() if not G.node[n]['val'] and not G.node[n]['test']]\n",
    "    test_ids = [n for n in G.nodes() if G.node[n]['test']]\n",
    "    val_ids = [n for n in G.nodes() if G.node[n]['val']]\n",
    "    train_labels = [labels[i] for i in train_ids]\n",
    "    test_labels = [labels[i] for i in test_ids]\n",
    "    val_labels = [labels[i] for i in val_ids]\n",
    "    feats = np.load(dataset_dir + \"/Network_p_0.016_w_0.003_1-feats.npy\")\n",
    "\n",
    "    ## Logistic gets thrown off by big counts, so log transform num comments and score\n",
    "    feats[:, 0] = np.log(feats[:, 0] + 1.0)\n",
    "    feats[:, 1] = np.log(feats[:, 1] - min(np.min(feats[:, 1]), -1))\n",
    "    feat_id_map = json.load(open(dataset_dir + \"/Network_p_0.016_w_0.003_1-id_map.json\"))\n",
    "    feat_id_map = {id: val for id, val in feat_id_map.items()}\n",
    "\n",
    "    train_index = [feat_id_map[id] for id in train_ids]\n",
    "    val_index = [feat_id_map[id] for id in val_ids]\n",
    "    test_index = [feat_id_map[id] for id in test_ids]\n",
    "    np.savez(dataset_dir + output_file, feats=feats, y_train=train_labels, y_val=val_labels, y_test=test_labels,\n",
    "             train_index=train_index,\n",
    "             val_index=val_index, test_index=test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
